<h1 id="overview">Overview</h1>
<p>We implemented rendering scenes with lighting! More importantly, we used several efficient algorithms (BVH, Russian
  Roulette for light bounces, adaptive sampling) that result in good-looking renders in a reasonable time.</p>
<h1 id="part-1">Part 1</h1>
<p><strong>Ray generation</strong>: We used the formula for the center of the virtual camera sensor
  $(-tan(\frac{hFov}{2}), -tan(\frac{hFov}{2}), -1)$, projected it to world space, and made sure the ray is normalized.
</p>
<p><strong>Triangle intersection</strong>:
  The triangle intersection algorithm checks if a ray intersects a triangle by calculating the triangle&#39;s edge
  vectors and using cross products to find vectors orthogonal to the ray direction and triangle edges. It then computes
  a denominator and intersection parameters (distance along the ray and Barycentric coordinates) to determine if the
  intersection point lies within the triangle and the ray&#39;s bounds. If an intersection occurs, the algorithm updates
  an intersection structure with the distance, the interpolated normal at the point, and material properties,
  efficiently determining the point of intersection and relevant surface information.</p>
<table>
  <thead>
    <tr>
      <th>a</th>
      <th>b</th>
      <th>c</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="./images/task1-empty.png" alt=""></td>
      <td><img src="./images/task1-sphere.png" alt=""></td>
      <td><img src="./images/task1-rabbit.png" alt=""></td>
    </tr>
  </tbody>
</table>
<h1 id="part-2">Part 2</h1>
<p>In the BVH algorithm, we divide space along an axis (X, Y, or Z) that gives us the most &quot;benefit&quot;, measured
  by the distance between the max and min centroid of bounding boxes.
  We continue recursively dividing space in this way for the &quot;left&quot; and &quot;right&quot; sides of the box
  until the <code>max_leaf_size</code> is reached.</p>
<p>Below we show three different .dae files rendered using BVH.
  The render time after implementing BVH is consistently well under 1 second and stays relatively consistent even as the
  scene is more complex.
  In contrast, the old render time without BVH grows proportionally to the number of primitives (~0.004s per primitive)
  and took a full 10 minutes to render Lucy.</p>
<table>
  <thead>
    <tr>
      <th>Image</th>
      <th># Primitives</th>
      <th>Avg rays/sec</th>
      <th>Render time</th>
      <th>Old render time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="./images/task2-cow" alt=""></td>
      <td>5856</td>
      <td>4.3389 mil</td>
      <td>0.0377s</td>
      <td>21.0473s</td>
    </tr>
    <tr>
      <td><img src="./images/task2-rabbit" alt=""></td>
      <td>28588</td>
      <td>4.6903 mil</td>
      <td>0.0463s</td>
      <td>100.9643s</td>
    </tr>
    <tr>
      <td><img src="./images/task2-lucy" alt=""></td>
      <td>133796</td>
      <td>4.2943 mil</td>
      <td>0.0401s</td>
      <td>595.8577s</td>
    </tr>
  </tbody>
</table>
<h1 id="part-3">Part 3</h1>
<p><strong>Hemisphere Sampling</strong> samples light uniformly from all directions over the hemisphere above the hit
  point.
  <strong>Light Sampling</strong> focuses on sampling light from directions that are more likely to contribute
  significantly to the final color seen at the hit point, reducing the variance.
</p>
<p>With one sample per pixel, the noise levels in the soft shadows are still high for both sampling methods. However,
  comparing the images below, uniform hemisphere sampling clearly shows more noise in soft shadows with fewer light rays
  due to the non-discriminatory approach of sample distribution. </p>
<table>
  <thead>
    <tr>
      <th></th>
      <th>s=1, not H</th>
      <th>Hemisphere</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>l=1</td>
      <td><img src="./images/task3-l-1.png" alt=""></td>
      <td><img src="./images/task3-l-1-H.png" alt=""></td>
    </tr>
    <tr>
      <td>l=4</td>
      <td><img src="./images/task3-l-4.png" alt=""></td>
      <td><img src="./images/task3-l-4-H.png" alt=""></td>
    </tr>
    <tr>
      <td>l=16</td>
      <td><img src="./images/task3-l-16.png" alt=""></td>
      <td><img src="./images/task3-l-16-H.png" alt=""></td>
    </tr>
    <tr>
      <td>l=64</td>
      <td><img src="./images/task3-l-64.png" alt=""></td>
      <td><img src="./images/task3-l-64-H.png" alt=""></td>
    </tr>
  </tbody>
</table>
<h1 id="part-4">Part 4</h1>
<p>To implement global illumination, we introduce indirect lighting using recursion.
  For each light bounce, we sample using the BSDF <code>sample_f()</code>,
  which gives the new direction to bounce to and the probability (pdf) of sampling in that direction.
  We recursively bounce until the maximum bounce depth is reached or, if enabled, it randomly terminates by Russian
  roulette, in which the direct lighting from <code>one_bounce_radiance</code> is returned.
  The radiance from each bounce is accumulated by multiplying the radiance from BSDF and the cosine of the angle between
  the incoming direction and the surface normal, normalized by the sampling pdf and adjusted for the Russian roulette
  termination probability.</p>
<table>
  <thead>
    <tr>
      <th>m0-5 no rr</th>
      <th>m0-5 rr</th>
      <th>m0-5-no-accum</th>
      <th>s#s</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="./images/task4-m0-norr.png" alt=""></td>
      <td><img src="./images/task4-m0-rr.png" alt=""></td>
      <td><img src="./images/task4-m0-accum.png" alt=""></td>
      <td><img src="./images/task4-s1.png" alt=""></td>
    </tr>
    <tr>
      <td><img src="./images/task4-m1-norr.png" alt=""></td>
      <td><img src="./images/task4-m1-rr.png" alt=""></td>
      <td><img src="./images/task4-m1-accum.png" alt=""></td>
      <td><img src="./images/task4-s2.png" alt=""></td>
    </tr>
    <tr>
      <td><img src="./images/task4-m2-norr.png" alt=""></td>
      <td><img src="./images/task4-m2-rr.png" alt=""></td>
      <td><img src="./images/task4-m2-accum.png" alt=""></td>
      <td><img src="./images/task4-s4.png" alt=""></td>
    </tr>
    <tr>
      <td><img src="./images/task4-m3-norr.png" alt=""></td>
      <td><img src="./images/task4-m3-rr.png" alt=""></td>
      <td><img src="./images/task4-m3-accum.png" alt=""></td>
      <td><img src="./images/task4-s8.png" alt=""></td>
    </tr>
    <tr>
      <td><img src="./images/task4-m4-norr.png" alt=""></td>
      <td><img src="./images/task4-m4-rr.png" alt=""></td>
      <td><img src="./images/task4-m4-accum.png" alt=""></td>
      <td><img src="./images/task4-s16.png" alt=""></td>
    </tr>
    <tr>
      <td><img src="./images/task4-m5-norr.png" alt=""></td>
      <td><img src="./images/task4-m5-rr.png" alt=""></td>
      <td><img src="./images/task4-m5-accum.png" alt=""></td>
      <td><img src="./images/task4-s64.png" alt=""></td>
    </tr>
    <tr>
      <td></td>
      <td><img src="./images/task4-m100-rr.png" alt=""></td>
      <td></td>
      <td><img src="./images/task4-s1024.png" alt=""></td>
    </tr>
  </tbody>
</table>
<h1 id="part-5">Part 5</h1>
<p>Adaptive sampling dynamically adjusts the number of rays traced per pixel based on the variance in the light
  contribution of the samples: it continues sampling until the confidence interval of the estimated radiance falls below
  a specified threshold, which minimizes noise while optimizing computational effort. In our implementation, after each
  batch of samples, it checks if the uncertainty in the average radiance is acceptable; if so, it stops sampling,
  otherwise it continues until the maximum samples are reached.</p>
<table>
  <thead>
    <tr>
      <th>Sample Rate</th>
      <th>Final Render</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="./images/task5-samplerate.png" alt=""></td>
      <td><img src="./images/task5-render.png" alt=""></td>
    </tr>
  </tbody>
</table>
<h1 id="part-6-optional-">Part 6 (Optional)</h1>
<p>Place all of the relevant extra credit portions in Part 6 of your writeup. We will not be grading extra credit
  written in other sections.
  Make sure you include everything mentioned for the writeup in the extra credit portions (screenshots, explanations).
  In general, explain your approach/method in detail, implementation details that are relevant, and relevant screenshots
  demonstrating that your fix works. Timing tables with speedup numbers would be appreciated if there are optimizations
  involved.</p>
vscode-remote://ssh-remote%2Bdoorplug/home/t/tr/trinityc/hw3-pathtracer-sp24-tifftracer/docs/index.html